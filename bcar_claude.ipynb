{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "# from langchain.document_loaders import UnstructuredPDFLoader\n",
    "# from langchain.document_transformers import EmbeddingsRedundantFilter\n",
    "# from langchain.retrievers import ContextualCompressionRetriever\n",
    "# from langchain.retrievers.document_compressors import DocumentCompressorPipeline, EmbeddingsFilter\n",
    "# from langchain.chains import RetrievalQA\n",
    "import os \n",
    "# from langchain.llms import GooglePalm\n",
    "# from langchain.embeddings import GooglePalmEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "# from langchain.document_loaders import UnstructuredPDFLoader\n",
    "# from langchain.document_transformers import EmbeddingsRedundantFilter\n",
    "# from langchain.retrievers import ContextualCompressionRetriever\n",
    "# from langchain.retrievers.document_compressors import DocumentCompressorPipeline, EmbeddingsFilter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "# from langchain.chains import RetrievalQA\n",
    "from tqdm import tqdm\n",
    "# from time import sleep\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader,PyPDFLoader\n",
    "# from langchain import PromptTemplate, LLMChain\n",
    "# from langchain.document_loaders import UnstructuredExcelLoader\n",
    "# from langchain.vectorstores import DocArrayInMemorySearch\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain import PromptTemplate\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain.document_loaders import JSONLoader\n",
    "# from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyD6lORTrf5wLPP6wR6keH6yhP2Kwd-A1r4\"\n",
    "embedding_llm = GooglePalm(model_name=\"models/text-bison-001\", temperature= 0)\n",
    "embeddings = GooglePalmEmbeddings(model_name=\"models/embedding-gecko-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"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\"\n",
    "claude_models = [\"claude-instant-1\",\"claude-2\"]\n",
    "llm = ChatAnthropic(model=claude_models[1],temperature= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "institute = \"Bank of Montreal (BMO)\"\n",
    "institute_type = \"Full Form\"\n",
    "all_documents = {\n",
    "    \"BCAR\"                              :   {\"data\":\"./data/Basel Capital Adequacy Reporting (BCAR) 2023 (2).pdf\",\"index\":\"Basel Capital Adequacy Reporting (BCAR) 2023 (2)_index\"},\n",
    "    \"Bank of Montreal (BMO)\"            :   {\"data\":\"./data/bmo_ar2022 (2).pdf\",\"index\":\"bmo_ar2022 (2)_index\"},\n",
    "    \"Versa Bank (VB)\"                   :   {\"data\":\"./data/Versa bank\",\"index\":\"Versa bank_index\"},\n",
    "    \"National Bank of Canada (NBC)\"     :   {\"data\":\"./data/NATIONAL BANK OF CANADA_ 2022 Annual Report (1).pdf\",\"index\":\"NATIONAL BANK OF CANADA_ 2022 Annual Report (1)_index\"},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file_name,file_info in tqdm(all_documents.items()):\n",
    "#     if file_info[\"data\"].endswith(\".pdf\"):\n",
    "#         loader = PyPDFLoader(file_path=file_info[\"data\"])\n",
    "#     else:\n",
    "#         loader = DirectoryLoader(path=file_info[\"data\"],glob=\"**/*.pdf\")\n",
    "#     documents = loader.load()\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=0,length_function = len)\n",
    "#     split_documents = text_splitter.split_documents(documents)\n",
    "#     docsearch = FAISS.from_documents(split_documents, embeddings)\n",
    "#     docsearch.save_local(folder_path='./PALM_FAISS_VS', index_name=file_info[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(path):\n",
    "    if path.endswith(\".pdf\"):\n",
    "        doc = PyPDFLoader(file_path=path)\n",
    "    else:\n",
    "        doc = DirectoryLoader(path=path,glob=\"**/*.pdf\")\n",
    "    document = doc.load()\n",
    "    context = \"\\n\\n\".join([document[i].page_content for i in range(len(document))])\n",
    "    return context\n",
    "\n",
    "def get_relevant_docs(query,doc):\n",
    "    relevant_docs = doc.similarity_search(query,k=300)\n",
    "    return \"\\n\".join([relevant_docs[i].page_content for i in range(len(relevant_docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 350000\n",
    "bank_db = load_doc(all_documents[institute][\"data\"])[:k]\n",
    "bcar_db = load_doc(all_documents[\"BCAR\"][\"data\"])[:k]\n",
    "# bank_db = FAISS.load_local(folder_path='./PALM_FAISS_VS', embeddings=embeddings, index_name=all_documents[institute][\"index\"])\n",
    "# bcar_db = FAISS.load_local(folder_path='./PALM_FAISS_VS', embeddings=embeddings, index_name=\"Basel Capital Adequacy Reporting (BCAR) 2023 (2)_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = {\n",
    "    f\"{institute} Annual Report\"                :   bank_db,\n",
    "    \"Basel Capital Adequacy Reporting (BCAR)\"   :   bcar_db,\n",
    "    }\n",
    "\n",
    "def compare_answer(question,docs):\n",
    "    retrival_agent = ChatAnthropic(model=claude_models[1],temperature= 0)\n",
    "    retrival_system_template = \"\"\"You are a helpful assistant, You need to extract as much text as you can which is relater or relevant to the answer of the user question from the context provided.\n",
    "Do not try to answer the question, just extract the text relevant to the answer of the user question.\n",
    "Use the following context (delimited by <ctx></ctx>) for finding out the relevant text:\n",
    "\n",
    "<ctx>\n",
    "{context}\n",
    "</ctx>\"\"\"\n",
    "    \n",
    "    retrival_system_prompt = SystemMessagePromptTemplate.from_template(template=retrival_system_template)\n",
    "    messages = [retrival_system_prompt,HumanMessage(content=question)]\n",
    "    compare_chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    \n",
    "    summary = {}\n",
    "    for doc_name,doc_db in tqdm(docs.items()):\n",
    "        # context = get_relevant_docs(question,doc_db)\n",
    "        context = doc_db\n",
    "        summary[doc_name] = retrival_agent(compare_chat_prompt.format_prompt(context=context).to_messages()).content\n",
    "\n",
    "    compare_context = \"\\n\\n\".join([f\"Relevant points from {doc_name}:\\n\\n{doc_summary}\" for doc_name,doc_summary in summary.items()])\n",
    "    \n",
    "    compare_agent = ChatAnthropic(model=claude_models[1],temperature= 0)\n",
    "    compare_system_template = \"\"\"You are a helpful chatbot who has to answer question of a user from the institute {institute} which comes under the BCAR {institute_type} section.\n",
    "You will be given relevant points from various documents that will help you answer the user question.\n",
    "Below is a list of relevant points along with the name of the document from where thoes points are from.\n",
    "Consider all the documents provided to you and answer the question by choosing all the relevant points to the question.\n",
    "You might have to compare points from more than one document to answer the question.\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "    compare_system_prompt = SystemMessagePromptTemplate.from_template(template=compare_system_template)\n",
    "    messages = [compare_system_prompt,HumanMessage(content=question)]\n",
    "    compare_chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "    response = compare_agent(compare_chat_prompt.format_prompt(institute=institute,institute_type=institute_type,question=question,context=compare_context).to_messages()).content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:10<00:00, 65.41s/it]\n"
     ]
    }
   ],
   "source": [
    "question = f\"Based on the fiscal year-end mentioned in {institute}'s Annual Report when should it submit BCAR?\"\n",
    "response = compare_answer(question,docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the fiscal year-end mentioned in Bank of Montreal (BMO)'s Annual Report when should it submit BCAR? \n",
      "\n",
      "  Based on the information provided, Bank of Montreal (BMO) should submit BCAR within 90 days after its fiscal year-end of October 31, which would be by late January of the following year. \n",
      "\n",
      "Specifically, the key relevant points are:\n",
      "\n",
      "- BMO's fiscal year ends on October 31, as stated in its Annual Report.\n",
      "\n",
      "- As a federally regulated financial institution, BMO is required to submit regulatory capital filings including BCAR to OSFI. \n",
      "\n",
      "- OSFI requires BCAR to be submitted within 90 days of fiscal year-end.\n",
      "\n",
      "- Therefore, given BMO's fiscal year ends October 31, it should submit its annual BCAR by late January of the following year. \n",
      "\n",
      "So in summary, BMO should submit its annual BCAR by late January each year, based on its October 31 fiscal year-end date mentioned in its Annual Report.\n"
     ]
    }
   ],
   "source": [
    "print(question,\"\\n\\n\",response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:14<00:00, 67.21s/it]\n"
     ]
    }
   ],
   "source": [
    "question = f\"Which schedules does bank {institute} have to send to OSFI for BCAR Credit Risk?\"\n",
    "response = compare_answer(question,docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which schedules does bank Bank of Montreal (BMO) have to send to OSFI for BCAR Credit Risk? \n",
      "\n",
      "  Based on the provided context and relevant points, the schedules that Bank of Montreal (BMO) would have to send to OSFI for BCAR Credit Risk are:\n",
      "\n",
      "- Schedule 2A - Wholesale Credit Risk - AIRB \n",
      "- Schedule 2B - Retail Credit Risk - AIRB\n",
      "- Schedule 2C - Equity Exposures - Simple Risk Weight Method\n",
      "- Schedule 2D - Securitization Exposures\n",
      "- Schedule 2E - Other Assets\n",
      "- 40 series schedules for credit RWA under the Standardized Approach (for certain Canadian and U.S. portfolios using this approach)\n",
      "- 50 series schedules for credit RWA under the IRB Approach\n",
      "- 60 series schedules for securitization exposures\n",
      "- 70.030 for derivative contracts \n",
      "- 70.040 for central counterparty credit risk\n",
      "- 80.010 for credit valuation adjustments\n",
      "\n",
      "The key schedules are 2A to 2E based on BMO's use of the AIRB approach, as well as the applicable 40 and 50 series schedules for portfolios under the Standardized approach. The additional schedules cover other credit risk components like securitizations, derivatives, CCPs, and CVAs.\n"
     ]
    }
   ],
   "source": [
    "print(question,\"\\n\\n\",response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = {\n",
    "#     f\"{institute} Annual Report\"                :   bank_txt,\n",
    "#     \"Basel Capital Adequacy Reporting (BCAR)\"   :   bcar_txt,\n",
    "#     }\n",
    "\n",
    "# def compare_answer(question,docs):\n",
    "#     retrival_template = \"\"\"You are a helpful assistant, extract as much text as you can from the context related to answer the following question \"{question}\".\n",
    "#     Use the following context (delimited by <ctx></ctx>) for finding out the necessary point:\n",
    "\n",
    "#     <ctx>\n",
    "#     {context}\n",
    "#     </ctx>\n",
    "\n",
    "#     Answer:\"\"\"\n",
    "\n",
    "#     retrival_prompt = PromptTemplate(input_variables=[\"question\", \"context\"],template=retrival_template)\n",
    "    \n",
    "#     summary = dict()\n",
    "#     for doc_name,doc_txt in docs.items():\n",
    "#         agent = LLMChain(prompt=retrival_prompt,llm=llm)\n",
    "#         context = doc_txt[:300000]\n",
    "#         summary[doc_name] = agent.run({\"question\":question,\"context\":context})\n",
    "\n",
    "#     compare_context = \"\\n\\n\".join([f\"Relevant points from {doc_name}:\\n\\n{doc_summary}\" for doc_name,doc_summary in summary.items()])\n",
    "\n",
    "#     print(compare_context,\"\\n\\n\\n\")\n",
    "    \n",
    "#     compare_agent = ChatAnthropic(model=claude_models[1],temperature= 0)\n",
    "\n",
    "#     compare_system_template = \"\"\"You are a helpful chatbot who has to answer question of a user from the institute {institute} which comes under the BCAR {institute_type} section.\n",
    "# You will be given relevant points from various documents that will help you answer the user question.\n",
    "# Below is a list of relevant points along with the name of the document from where thoes points are from.\n",
    "# Consider all the documents provided to you and answer the question by choosing all the relevant points to the question.\n",
    "# You might have to compare points from more than one document to answer the question.\n",
    "\n",
    "# {context}\"\"\"\n",
    "\n",
    "#     compare_system_prompt = SystemMessagePromptTemplate(prompt=compare_system_prompt_template)\n",
    "    \n",
    "#     messages = [compare_system_prompt,HumanMessage(question)]\n",
    "    \n",
    "#     compare_chat_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "#     response = compare_agent(compare_chat_prompt.format_prompt(institute=institute,institute_type=institute_type,question=question,context=compare_context).to_messages()).content\n",
    "\n",
    "#     compare_prompt = PromptTemplate(input_variables=[\"institute\",\"institute_type\",\"question\", \"context\"],template=compare_template)\n",
    "\n",
    "#     compare_agent = LLMChain(prompt=compare_prompt,llm=llm,verbose=True)\n",
    "    \n",
    "#     response = compare_agent.run({\"institute\":institute,\"institute_type\":institute_type,\"question\":question,\"context\":compare_context})\n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = compare_answer(\"Based on the fiscal year-end mentioned in Versa Bank's Annual Report when should Versa Bank submit BCAR\",docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Based on the fiscal year-end mentioned in Bank of Montreal's Annual Report when should BMO submit BCAR?\",\"\\n\\n\",response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bcar_doc = PyPDFLoader(f\"./data/bmo_ar2022 (2).pdf\")\n",
    "# bacr_document = bcar_doc.load()\n",
    "# sum(len(bacr_document[i].page_content) for i in range(len(bacr_document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = \"\\n\\n\".join([bacr_document[i].page_content for i in range(len(bacr_document))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_template = f\"\"\"You are a helpful chatbot who has to answer question of a user from the institute {institute} which comes under the BCAR {institute_type} section.\n",
    "# Use the below context to answer the question.\"\"\"+\"\"\"\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Answer:\"\"\"\n",
    "\n",
    "# compare_prompt = PromptTemplate(input_variables=[\"context\",\"question\"],template=compare_template)\n",
    "\n",
    "# compare_agent = LLMChain(prompt=compare_prompt,llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What are the different approaches used to file credit risk?\"\n",
    "# response = compare_agent.run({\"context\":context,\"question\":question})\n",
    "# # messages = [\n",
    "# #     HumanMessage(\n",
    "# #         content=question,\n",
    "# #     )\n",
    "# # ]\n",
    "# # llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = {\n",
    "#     f\"{institute} Annual Report\"                :   bank_db,\n",
    "#     \"Basel Capital Adequacy Reporting (BCAR)\"   :   bcar_db,\n",
    "#     }\n",
    "\n",
    "# def compare_answer(question,docs):\n",
    "#     retrival_template = \"\"\"You are a helpful assistant, extract as much text as you can from the context related to answer the following question \"{question}\".\n",
    "#     Use the following context (delimited by <ctx></ctx>) for finding out the necessary point:\n",
    "\n",
    "#     <ctx>\n",
    "#     {context}\n",
    "#     </ctx>\n",
    "\n",
    "#     Answer:\"\"\"\n",
    "\n",
    "#     retrival_prompt = PromptTemplate(input_variables=[\"question\", \"context\"],template=retrival_template)\n",
    "#     summary = dict()\n",
    "#     for doc_name,doc_db in docs.items():\n",
    "#         agent = RetrievalQA.from_chain_type(llm = llm,\n",
    "#             chain_type='stuff', # 'stuff', 'map_reduce', 'refine', 'map_rerank'\n",
    "#             retriever=doc_db.as_retriever(),\n",
    "#             verbose=False,\n",
    "#             chain_type_kwargs={\n",
    "#             \"verbose\":False,\n",
    "#             \"prompt\": retrival_prompt,\n",
    "#             \"memory\": ConversationBufferMemory(\n",
    "#                 input_key=\"question\"),\n",
    "#         })\n",
    "#         summary[doc_name] = agent.run(question)\n",
    "\n",
    "#     compare_context = \"\\n\\n\".join([f\"Relevant points from {doc_name}:\\n\\n{doc_summary}\" for doc_name,doc_summary in summary.items()])\n",
    "\n",
    "#     print(compare_context)\n",
    "\n",
    "#     compare_template = f\"\"\"You are a helpful chatbot who has to answer question of a user from the institute {institute} which comes under the BCAR {institute_type} section.\n",
    "# You will be given relevant points from various documents that will help you answer the user question.\n",
    "# Below is a list of relevant points along with the name of the document from where thoes points are from.\n",
    "# Consider all the documents provided to you and answer the question by choosing all the relevant points to the question.\n",
    "# You might have to compare points from more than one document to answer the question.\n",
    "# \"\"\"+\"\"\"\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Answer:\"\"\"\n",
    "\n",
    "#     compare_prompt = PromptTemplate(input_variables=[\"question\", \"context\"],template=compare_template)\n",
    "\n",
    "#     compare_agent = LLMChain(prompt=compare_prompt,llm=llm,verbose=True)\n",
    "    \n",
    "#     response = compare_agent.run({\"question\":question,\"context\":compare_context}))\n",
    "#     return response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
